import asyncio
# On Windows, this line handles specific event loop requirements for Playwright.
# On Linux/macOS, it can often be omitted or set differently.
try:
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
except AttributeError:
    pass # Ignore if policy is not available (e.g., non-Windows)

import time
import sys # <-- ì‹œìŠ¤í…œ ë¡œê¹…ì„ ìœ„í•´ ì¶”ê°€
from typing import Dict, Any, List, Optional

# Playwright and BeautifulSoup are imported inside the class methods 
# or assumed to be available globally in this runnable script.
from bs4 import BeautifulSoup
# The playwright import is intentionally left inside fetch_html 
# as provided in the original source, but the library must be installed.


# --- Original MyResultScraper Class ---
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; MyRunnerViewer/1.0; +https://example.com)"
}

class MyResultScraper:
    """
    MyResultScraper í´ë˜ìŠ¤ëŠ” Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ë¡œë“œëœ 
    ì›¹ í˜ì´ì§€ì—ì„œ ë‹¬ë¦¬ê¸° ê²½ì£¼ ê²°ê³¼ë¥¼ ìŠ¤í¬ë©í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    def __init__(self, base: str = "https://www.myresult.co.kr", delay_sec: float = 0.8, timeout: int = 15):
        self.base = base.rstrip("/")
        self.delay_sec = delay_sec
        self.timeout = timeout

    def fetch_html(self, race_id: int, runner_id: int) -> Optional[str]:
        """Playwrightë¡œ ë Œë”ë§ëœ HTML ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base}/{race_id}/{runner_id}"
        print(f"[{race_id}/{runner_id}] URL ì ‘ì† ì‹œë„: {url}", file=sys.stderr) # ë¡œê·¸ ê°•í™”
        
        try:
            from playwright.sync_api import sync_playwright
            
            with sync_playwright() as p:
                # ğŸš¨ Docker í™˜ê²½ ì‹¤í–‰ì„ ìœ„í•´ --no-sandbox ì˜µì…˜ ì¶”ê°€
                browser = p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox'] # <-- í•µì‹¬ ìˆ˜ì •
                )
                page = browser.new_page()
                page.goto(url, timeout=self.timeout * 1000)
                
                # JS ë°ì´í„° ë¡œë“œ ëŒ€ê¸°: 
                print(f"[{race_id}/{runner_id}] ë°ì´í„° ì…€ë ‰í„° ëŒ€ê¸° ì¤‘...", file=sys.stderr)
                # ë°ì´í„°ê°€ ë¡œë“œëœ í…Œì´ë¸” í–‰ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. (ìµœëŒ€ 8ì´ˆ)
                page.wait_for_selector("div.table-row.ant-row", timeout=8000)
                
                html = page.content()
                browser.close()
                print(f"[{race_id}/{runner_id}] HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ.", file=sys.stderr)
                return html
        except Exception as e:
            # ğŸš¨ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ë¥¼ sys.stderrë¡œ ì¶œë ¥í•˜ì—¬ ë¡œê·¸ í™•ì¸ì´ ìš©ì´í•˜ë„ë¡ í•¨
            print(f"[{race_id}/{runner_id}] [Playwright ì˜¤ë¥˜] HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", file=sys.stderr)
            return None

    def parse_runner(self, html: str) -> Dict[str, Any]:
        """ê°€ì ¸ì˜¨ HTMLì„ ë¶„ì„í•˜ì—¬ ì£¼ì ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html, "lxml")
        rows = soup.select("div.table-row.ant-row")
        sections: List[Dict[str, Any]] = []

        # ì„¹ì…˜ë³„ ê¸°ë¡ ì¶”ì¶œ (ì¤‘ê°„ ê¸°ë¡)
        for row in rows:
            # ëª¨ë“  ë°ì´í„°ë¥¼ ant-col-6 í´ë˜ìŠ¤ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            cols = [div.get_text(strip=True) for div in row.select("div.ant-col.ant-col-6")]
            if not cols:
                continue
            
            # ì»¬ëŸ¼ ì¸ë±ìŠ¤ í™•ì¸ ë° í• ë‹¹
            sec = {
                "section": cols[0] if len(cols) > 0 else "", # êµ¬ê°„ ì´ë¦„
                "pass_time": cols[1] if len(cols) > 1 else "", # í†µê³¼ ì‹œê°„
                "split_time": cols[2] if len(cols) > 2 else "", # êµ¬ê°„ ê¸°ë¡
                "total_time": cols[3] if len(cols) > 3 else "", # ëˆ„ì  ì‹œê°„
            }
            sections.append(sec)

        # -----------------------------
        # ì´ë¦„, ì„±ë³„, ë°°ë²ˆ ì¶”ì¶œ (í˜ì´ì§€ ìƒë‹¨ ë©”íƒ€ ì •ë³´)
        # -----------------------------
        name = ""
        gender = ""
        bib_no = ""
        try:
            # ì´ë¦„: div.ant-card-meta-title
            name_tag = soup.select_one("div.ant-card-meta-detail > div.ant-card-meta-title") 
            if name_tag:
                name = name_tag.get_text(strip=True)

            # ì„±ë³„/ë²ˆí˜¸: div.ant-card-meta-description (ì˜ˆ: ë‚¨ì | #1060)
            desc_tag = soup.select_one("div.ant-card-meta-detail > div.ant-card-meta-description")
            if desc_tag:
                desc_text = desc_tag.get_text(strip=True)
                parts = [p.strip() for p in desc_text.split("|")]
                if len(parts) >= 1:
                    gender = parts[0].strip()
                if len(parts) >= 2:
                    bib_no = parts[1].replace("#", "").strip()
        except Exception as e:
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê¸°ë³¸ê°’ ì‚¬ìš©
            print(f"íŒŒì‹± ì¤‘ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            pass

        # ëŒ€íšŒëª… ì¶”ì¶œ
        event_name = ""
        try:
            event_tag = soup.find("div", class_="ant-card-head-title")
            if event_tag:
                event_name = event_tag.get_text(strip=True)
        except Exception:
            pass
        
        return {
            "name": name or "ì´ë¦„ì—†ìŒ",
            "gender": gender,
            "bib_no": bib_no,
            "event_name": event_name,
            "sections": sections,
        }
        
    def get_runner(self, race_id: int, runner_id: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ì£¼ì ê¸°ë¡ì„ ê°€ì ¸ì™€ íŒŒì‹±ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # ì§€ì—° ì‹œê°„ ì ìš©
        time.sleep(self.delay_sec) 
        
        html = self.fetch_html(race_id, runner_id)
        if not html:
            return {"runner_id": runner_id, "error": "fetch_failed"}
        
        parsed = self.parse_runner(html)
        parsed["runner_id"] = runner_id
        return parsed

# ------------------------------------
# ë°ëª¨ ì‹¤í–‰ ë¡œì§ (ë³€ê²½ ì—†ìŒ)
# ------------------------------------
if __name__ == "__main__":
    # --- ì£¼ì˜ ---
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” Race IDì™€ Runner IDë¥¼ ì‚¬ìš©í•´ì•¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì•„ë˜ IDëŠ” ì˜ˆì‹œì´ë¯€ë¡œ, ìŠ¤í¬ë˜í¼ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ 
    # ì‹¤ì œ MyResult í˜ì´ì§€ì—ì„œ ìœ íš¨í•œ IDë¥¼ ì°¾ì•„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    EXAMPLE_RACE_ID = 1111 # ì—¬ê¸°ì— ì‹¤ì œ ë ˆì´ìŠ¤ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.
    EXAMPLE_RUNNER_ID = 1001 # ì—¬ê¸°ì— ì‹¤ì œ ì£¼ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.

    # 1. Scraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¸°ë³¸ 0.8ì´ˆ ë”œë ˆì´ ì ìš©)
    scraper = MyResultScraper(
        base="https://www.myresult.co.kr", 
        delay_sec=0.8, 
        timeout=15
    )

    print("--- ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")
    
    # 2. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    runner_data = scraper.get_runner(
        race_id=EXAMPLE_RACE_ID, 
        runner_id=EXAMPLE_RUNNER_ID
    )

    print("\n--- ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ---")
    
    # 3. ê²°ê³¼ ì¶œë ¥
    if "error" in runner_data:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {runner_data['error']}. ID ({EXAMPLE_RACE_ID}/{EXAMPLE_RUNNER_ID})ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        print(f"ëŒ€íšŒëª…: {runner_data['event_name']}")
        print(f"ì´ë¦„: {runner_data['name']} ({runner_data['gender']}, ë°°ë²ˆ #{runner_data['bib_no']})")
        print("-" * 30)
        
        if runner_data['sections']:
            print(f"{'êµ¬ê°„ëª…':<10} | {'í†µê³¼ ì‹œê°„':<10} | {'êµ¬ê°„ ê¸°ë¡':<10} | {'ëˆ„ì  ì‹œê°„':<10}")
            print("-" * 30)
            for sec in runner_data['sections']:
                print(
                    f"{sec['section']:<10} | {sec['pass_time']:<10} | "
                    f"{sec['split_time']:<10} | {sec['total_time']:<10}"
                )
        else:
            print("ì„¹ì…˜ë³„ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("\n--- ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ---")
